{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJb2la6wSG89",
        "outputId": "2e5e9fdd-eb1f-4b3e-a35b-d21f72240a34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned Q-table:\n",
            "[[ 6.561       7.29      ]\n",
            " [ 6.561       8.1       ]\n",
            " [ 7.29        9.        ]\n",
            " [ 8.09999994 10.        ]\n",
            " [ 0.          0.        ]]\n",
            "Best actions for each state:\n",
            "[1 1 1 1 0]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Constants\n",
        "LEFT = 0\n",
        "RIGHT = 1\n",
        "GRID_SIZE = 5  # Number of cells in the 1D grid\n",
        "GOAL_STATE = GRID_SIZE - 1\n",
        "EPISODES = 1000\n",
        "LEARNING_RATE = 0.1\n",
        "DISCOUNT_FACTOR = 0.9\n",
        "EXPLORATION_PROB = 0.3\n",
        "\n",
        "# Q-learning function\n",
        "def q_learning(grid_size, episodes, learning_rate, discount_factor, exploration_prob):\n",
        "    # Initialize Q-table with zeros\n",
        "    q_table = np.zeros((grid_size, 2))  # 2 actions: Left and Right\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = 0  # Start from the leftmost cell\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Choose an action using epsilon-greedy strategy\n",
        "            if np.random.uniform(0, 1) < exploration_prob:\n",
        "                action = np.random.choice([LEFT, RIGHT])\n",
        "            else:\n",
        "                action = np.argmax(q_table[state, :])\n",
        "\n",
        "            # Perform the action and observe the reward and new state\n",
        "            if action == RIGHT:\n",
        "                next_state = min(state + 1, grid_size - 1)\n",
        "            else:\n",
        "                next_state = max(state - 1, 0)\n",
        "\n",
        "            # Define rewards\n",
        "            if next_state == GOAL_STATE:\n",
        "                reward = 10\n",
        "                done = True\n",
        "            else:\n",
        "                reward = 0\n",
        "\n",
        "            # Update the Q-value for the current state-action pair\n",
        "            q_table[state, action] = q_table[state, action] + \\\n",
        "                learning_rate * (reward + discount_factor * np.max(q_table[next_state, :]) - q_table[state, action])\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "    return q_table\n",
        "\n",
        "# Run Q-learning\n",
        "q_table = q_learning(GRID_SIZE, EPISODES, LEARNING_RATE, DISCOUNT_FACTOR, EXPLORATION_PROB)\n",
        "\n",
        "# Print the learned Q-table\n",
        "print(\"Learned Q-table:\")\n",
        "print(q_table)\n",
        "\n",
        "# Choose the best action for each state\n",
        "best_actions = np.argmax(q_table, axis=1)\n",
        "\n",
        "# Print the best actions for each state\n",
        "print(\"Best actions for each state:\")\n",
        "print(best_actions)\n"
      ]
    }
  ]
}